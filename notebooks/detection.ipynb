{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "import os, random, pickle, time\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch, torchvision\n",
    "from torch.optim import AdamW\n",
    "from torch.utils import data\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchmetrics.detection import MeanAveragePrecision\n",
    "import torchvision.transforms.functional as F\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/data/avramidi/tiny_vindr/\"\n",
    "train_path = path + \"train_images/\"\n",
    "test_path = path + \"test_images/\"\n",
    "annot_path = path + \"annotations/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(path):\n",
    "\n",
    "    train_path = path + \"train_images/\"\n",
    "    train_list = [os.path.join(train_path, img) for img in os.listdir(train_path)]\n",
    "\n",
    "    random.shuffle(train_list)\n",
    "    threshold = int(0.8 * len(train_list))\n",
    "    valid_list = train_list[threshold:]\n",
    "    train_list = train_list[:threshold]\n",
    "\n",
    "    test_path = path + \"test_images/\"\n",
    "    test_list = [os.path.join(test_path, img) for img in os.listdir(test_path)]\n",
    "\n",
    "    return train_list, valid_list, test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid, test = train_test_split(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_annotation_map(annotations):\n",
    "\n",
    "    annotation_map = {}\n",
    "    for _, row in annotations.iterrows():\n",
    "        boxes = annotation_map.get(row[\"image_id\"], [])\n",
    "        boxes.append(\n",
    "            (\n",
    "                row[\"lesion_type\"],\n",
    "                [\n",
    "                    row[\"xmin\"] if row[\"xmin\"] >= 0 else -1,\n",
    "                    row[\"ymin\"] if row[\"ymin\"] >= 0 else -1,\n",
    "                    row[\"xmax\"] if row[\"xmax\"] >= 0 else -1,\n",
    "                    row[\"ymax\"] if row[\"ymax\"] >= 0 else -1,\n",
    "                ],\n",
    "            )\n",
    "        )\n",
    "        annotation_map[row[\"image_id\"]] = boxes\n",
    "    return annotation_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_annotations = pd.read_csv(annot_path + \"train.csv\")\n",
    "test_annotations  = pd.read_csv(annot_path + \"test.csv\" )\n",
    "\n",
    "train_annotation_map = compute_annotation_map(train_annotations)\n",
    "\n",
    "train_ids = [image.split(\"/\")[-1].split(\".\")[0] for image in train]\n",
    "valid_ids = [image.split(\"/\")[-1].split(\".\")[0] for image in valid]\n",
    "\n",
    "train_dict = {image: train_annotation_map[image] for image in train_ids}\n",
    "valid_dict = {image: train_annotation_map[image] for image in valid_ids}\n",
    "test_dict  = compute_annotation_map(test_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_image_ids = list(train_dict.keys())\n",
    "all_image_ids.extend(list(valid_dict.keys()))\n",
    "all_image_ids.extend(list(test_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_map = {\n",
    "    \"No finding\": 0,\n",
    "    \"Disc space narrowing\": 1,\n",
    "    \"Foraminal stenosis\": 2,\n",
    "    \"Osteophytes\": 3,\n",
    "    \"Spondylolysthesis\": 4,\n",
    "    \"Surgical implant\": 5,\n",
    "    \"Vertebral collapse\": 6,\n",
    "    \"Other lesions\": 7,\n",
    "}\n",
    "image_id_map = {img_id: i + 1 for i, img_id in enumerate(all_image_ids)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Compose:\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, img, boxes):\n",
    "        for t in self.transforms:\n",
    "            img, boxes = t(img, boxes)\n",
    "        return img, boxes\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        format_string = self.__class__.__name__ + \"(\"\n",
    "        for t in self.transforms:\n",
    "            format_string += \"\\n\"\n",
    "            format_string += f\"    {t}\"\n",
    "        format_string += \"\\n)\"\n",
    "        return format_string\n",
    "\n",
    "\n",
    "class RandomHorizontalFlip:\n",
    "    def __init__(self, p=0.5):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, img, boxes):\n",
    "        if torch.rand(1) < self.p:\n",
    "            img_width = img.size[0]\n",
    "\n",
    "            boxes[:, 0] = img_width - boxes[:, 0]\n",
    "            boxes[:, 2] = img_width - boxes[:, 2]\n",
    "            boxes_w = torch.abs(boxes[:, 0] - boxes[:, 2])\n",
    "\n",
    "            boxes[:, 0] -= boxes_w\n",
    "            boxes[:, 2] += boxes_w\n",
    "            \n",
    "            return F.hflip(img), boxes\n",
    "        return img, boxes\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.__class__.__name__}(p={self.p})\"\n",
    "\n",
    "\n",
    "class ToTensor:\n",
    "    def __call__(self, img, boxes):\n",
    "        return F.to_tensor(img), boxes\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.__class__.__name__}()\"\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpineObjectDetection(data.Dataset):\n",
    "    def __init__(self, root_dir, annotation_map, anomaly_map, image_id_map, transform):\n",
    "        self.img_paths = root_dir\n",
    "        self.img_paths.sort()\n",
    "        self.image_ids = [image.split(\"/\")[-1].split(\".\")[0] for image in self.img_paths]\n",
    "\n",
    "        self.annotation_map = annotation_map\n",
    "        self.anomaly_map = anomaly_map\n",
    "        self.image_id_map = image_id_map\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.img_paths[idx])\n",
    "        image_id = self.image_ids[idx]\n",
    "\n",
    "        image_width, image_height = image.size\n",
    "        ratio = image_width / image_height\n",
    "        transformed_w = np.random.randint(640, 800)\n",
    "        transformed_h = int(transformed_w / ratio)\n",
    "\n",
    "        image = transforms.Resize((transformed_w, transformed_h))(image)\n",
    "\n",
    "        labels, boxes, area = [], [], []\n",
    "        for label, box in self.annotation_map[image_id]:\n",
    "            labels.append(self.anomaly_map[label])\n",
    "\n",
    "            if box[0] == -1 and box[1] == -1:\n",
    "                boxes.append([0, 0, 1, 1])\n",
    "                #boxes.append([0, 0, transformed_w, transformed_h])\n",
    "            else:\n",
    "                boxes.append(\n",
    "                    [\n",
    "                        (box[0] / image_width) * transformed_w,\n",
    "                        (box[1] / image_height) * transformed_h,\n",
    "                        (box[2] / image_width) * transformed_w,\n",
    "                        (box[3] / image_height) * transformed_h,\n",
    "                    ]\n",
    "                )\n",
    "            area.append((boxes[-1][2] - boxes[-1][0]) * (boxes[-1][3] - boxes[-1][1]))\n",
    "\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        area = torch.as_tensor(area, dtype=torch.float32)\n",
    "\n",
    "        image, boxes = self.transform(image, boxes)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"area\"] = area\n",
    "        target[\"image_id\"] = torch.tensor(self.image_id_map[image_id])\n",
    "\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = Compose([RandomHorizontalFlip(p=0.5), ToTensor()])\n",
    "\n",
    "train_dataset = SpineObjectDetection(train, train_dict, anomaly_map, image_id_map, ToTensor())\n",
    "valid_dataset = SpineObjectDetection(valid, valid_dict, anomaly_map, image_id_map, ToTensor())\n",
    "test_dataset  = SpineObjectDetection(test , test_dict , anomaly_map, image_id_map, ToTensor())\n",
    "\n",
    "train_loader = data.DataLoader(train_dataset, 2, shuffle=True, collate_fn=collate_fn, num_workers=8)\n",
    "valid_loader = data.DataLoader(valid_dataset, 2, shuffle=True, collate_fn=collate_fn, num_workers=8)\n",
    "test_loader  = data.DataLoader(test_dataset , 2, shuffle=True, collate_fn=collate_fn, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveBestModel:\n",
    "    def __init__(self, model_name, path=\"/\"):\n",
    "        self.best_valid_loss = float(\"inf\")\n",
    "        self.path = path\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def update(self, model, current_valid_loss):\n",
    "        if current_valid_loss < self.best_valid_loss:\n",
    "            self.best_valid_loss = current_valid_loss\n",
    "            torch.save(model, os.path.join(self.path, self.model_name))\n",
    "            print(f\"Saved Model. Best validation loss: {self.best_valid_loss}\")\n",
    "\n",
    "    def fetch(self):\n",
    "        return torch.load(os.path.join(self.path, self.model_name))\n",
    "\n",
    "\n",
    "class LossHistory:\n",
    "    def __init__(self, path, file_name):\n",
    "        self.loss = {\n",
    "            \"total_loss\": [],\n",
    "            \"classifier_loss\": [],\n",
    "            \"box_reg_loss\": [],\n",
    "            \"objectness_loss\": [],\n",
    "            \"rpn_box_reg_loss\": [],\n",
    "        }\n",
    "        self.file_name = file_name\n",
    "        self.path = path\n",
    "\n",
    "    def update(self, total_loss, classifier_loss, box_reg_loss, objectness_loss, rpn_box_reg_loss):\n",
    "        self.loss[\"total_loss\"].append(total_loss)\n",
    "        self.loss[\"classifier_loss\"].append(classifier_loss)\n",
    "        self.loss[\"box_reg_loss\"].append(box_reg_loss)\n",
    "        self.loss[\"objectness_loss\"].append(objectness_loss)\n",
    "        self.loss[\"rpn_box_reg_loss\"].append(rpn_box_reg_loss)\n",
    "\n",
    "    def save(self):\n",
    "        save_object(self.path, self.file_name, self.loss)\n",
    "\n",
    "    def load(self):\n",
    "        self.loss = load_object(self.path, self.file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_object(path, file_name, obj):\n",
    "    with open(os.path.join(path, file_name + \".pickle\"), \"wb\") as file:\n",
    "        pickle.dump(obj, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def load_object(path, file_name):\n",
    "    with open(os.path.join(path, file_name + \".pickle\"), \"rb\") as file:\n",
    "        return pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_faster_rcnn_model(num_classes, trainable_backbone_layers=3):\n",
    "\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(\n",
    "        pretrained=True,\n",
    "        trainable_backbone_layers=trainable_backbone_layers,\n",
    "        min_size=640,\n",
    "        max_size=2699,\n",
    "    )\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    \n",
    "    model.train()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, optim):\n",
    "    (\n",
    "        running_total_loss,\n",
    "        running_loss_classifier,\n",
    "        running_loss_box_reg,\n",
    "        running_loss_objectness,\n",
    "        running_loss_rpn_box_reg,\n",
    "    ) = (0, 0, 0, 0, 0)\n",
    "    for images, targets in tqdm(train_loader, disable=False):\n",
    "        images_device = list(image.to(device) for image in images)\n",
    "        targets_device = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        optim.zero_grad()\n",
    "        with torch.set_grad_enabled(True):\n",
    "            loss_dict = model.forward(images_device, targets_device)\n",
    "            loss = sum(loss for loss in loss_dict.values())\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "        running_total_loss += loss.item()\n",
    "        running_loss_classifier += loss_dict[\"loss_classifier\"].item()\n",
    "        running_loss_box_reg += loss_dict[\"loss_box_reg\"].item()\n",
    "        running_loss_objectness += loss_dict[\"loss_objectness\"].item()\n",
    "        running_loss_rpn_box_reg += loss_dict[\"loss_rpn_box_reg\"].item()\n",
    "\n",
    "    epoch_total_loss = running_total_loss / len(train_loader)\n",
    "    epoch_loss_classifier = running_loss_classifier / len(train_loader)\n",
    "    epoch_loss_box_reg = running_loss_box_reg / len(train_loader)\n",
    "    epoch_loss_objectness = running_loss_objectness / len(train_loader)\n",
    "    epoch_loss_rpn_box_reg = running_loss_rpn_box_reg / len(train_loader)\n",
    "    print(\n",
    "        \"Total Loss: {:.6f}\\t Classifier Loss: {:.6f}\\t Box Reg Loss: {:.6f}\\t Objectness Loss: {:.6f}\\t RPN box Loss: {:.6f} \".format(\n",
    "            epoch_total_loss,\n",
    "            epoch_loss_classifier,\n",
    "            epoch_loss_box_reg,\n",
    "            epoch_loss_objectness,\n",
    "            epoch_loss_rpn_box_reg,\n",
    "        )\n",
    "    )\n",
    "    return (\n",
    "        epoch_total_loss,\n",
    "        epoch_loss_classifier,\n",
    "        epoch_loss_box_reg,\n",
    "        epoch_loss_objectness,\n",
    "        epoch_loss_rpn_box_reg,\n",
    "    )\n",
    "\n",
    "\n",
    "def evaluate_loss(model, loader):\n",
    "    (\n",
    "        running_total_loss,\n",
    "        running_loss_classifier,\n",
    "        running_loss_box_reg,\n",
    "        running_loss_objectness,\n",
    "        running_loss_rpn_box_reg,\n",
    "    ) = (0, 0, 0, 0, 0)\n",
    "    for images, targets in tqdm(loader, disable=False):\n",
    "        print(targets)\n",
    "        images_device = list(image.to(device) for image in images)\n",
    "        targets_device = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        model.train()\n",
    "        with torch.set_grad_enabled(False):\n",
    "            loss_dict = model.forward(images_device, targets_device)\n",
    "            loss = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        running_total_loss += loss.item()\n",
    "        running_loss_classifier += loss_dict[\"loss_classifier\"].item()\n",
    "        running_loss_box_reg += loss_dict[\"loss_box_reg\"].item()\n",
    "        running_loss_objectness += loss_dict[\"loss_objectness\"].item()\n",
    "        running_loss_rpn_box_reg += loss_dict[\"loss_rpn_box_reg\"].item()\n",
    "\n",
    "    epoch_total_loss = running_total_loss / len(loader)\n",
    "    epoch_loss_classifier = running_loss_classifier / len(loader)\n",
    "    epoch_loss_box_reg = running_loss_box_reg / len(loader)\n",
    "    epoch_loss_objectness = running_loss_objectness / len(loader)\n",
    "    epoch_loss_rpn_box_reg = running_loss_rpn_box_reg / len(loader)\n",
    "    print(\n",
    "        \"Total Loss: {:.6f}\\t Classifier Loss: {:.6f}\\t Box Reg Loss: {:.6f}\\t Objectness Loss: {:.6f}\\t RPN box Loss: {:.6f} \".format(\n",
    "            epoch_total_loss,\n",
    "            epoch_loss_classifier,\n",
    "            epoch_loss_box_reg,\n",
    "            epoch_loss_objectness,\n",
    "            epoch_loss_rpn_box_reg,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        epoch_total_loss,\n",
    "        epoch_loss_classifier,\n",
    "        epoch_loss_box_reg,\n",
    "        epoch_loss_objectness,\n",
    "        epoch_loss_rpn_box_reg,\n",
    "    )\n",
    "\n",
    "\n",
    "def evaluate_average_precision(model, loader):\n",
    "    metric = MeanAveragePrecision(class_metrics=True)\n",
    "    for images, targets in tqdm(loader, disable=False):\n",
    "        images_device = list(image.to(device) for image in images)\n",
    "        targets_device = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        model.eval()\n",
    "        with torch.set_grad_enabled(False):\n",
    "            preds = model.forward(images_device)\n",
    "            metric.update(preds, targets_device)\n",
    "    mAP = metric.compute()\n",
    "    print(mAP)\n",
    "    return mAP\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = create_faster_rcnn_model(num_classes=8, trainable_backbone_layers=3).to(device)\n",
    "optim = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)\n",
    "best_model = SaveBestModel(\"model_object.pt\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model, best_model, train_loader, valid_loader, optim, epochs, path=\".\", evaluate_map_every=5\n",
    "):\n",
    "    train_history = LossHistory(path, \"train_history\")\n",
    "    valid_history = LossHistory(path, \"valid_history\")\n",
    "    mAP_history = []\n",
    "\n",
    "    for i in range(1, epochs + 1):\n",
    "        start = time.time()\n",
    "        print(f\"\\nEpoch {i}:\")\n",
    "        print(\"-\" * 10)\n",
    "\n",
    "        ## Training\n",
    "        print(\"Train\")\n",
    "        (\n",
    "            total_loss,\n",
    "            classifier_loss,\n",
    "            box_reg_loss,\n",
    "            objectness_loss,\n",
    "            rpn_box_reg_loss,\n",
    "        ) = train_one_epoch(model, train_loader, optim)\n",
    "        train_history.update(\n",
    "            total_loss, classifier_loss, box_reg_loss, objectness_loss, rpn_box_reg_loss\n",
    "        )\n",
    "\n",
    "        ## Validation\n",
    "        print(\"\\nValidation\")\n",
    "        (\n",
    "            total_loss,\n",
    "            classifier_loss,\n",
    "            box_reg_loss,\n",
    "            objectness_loss,\n",
    "            rpn_box_reg_loss,\n",
    "        ) = evaluate_loss(model, valid_loader)\n",
    "        valid_history.update(\n",
    "            total_loss, classifier_loss, box_reg_loss, objectness_loss, rpn_box_reg_loss\n",
    "        )\n",
    "        best_model.update(model, total_loss)\n",
    "\n",
    "        ## Validation Eval mAP\n",
    "        print(\"\\nmAP Validation\")\n",
    "        if i % evaluate_map_every == 0:\n",
    "            mAP = evaluate_average_precision(model, valid_loader)\n",
    "            mAP_history.append(mAP)\n",
    "\n",
    "        train_history.save(),\n",
    "        valid_history.save()\n",
    "        save_object(path, \"map_history\", mAP_history)\n",
    "        print(\"\\n Time Elapsed Per Epoch: \", time.time() - start)\n",
    "    return best_model.fetch(), train_history, valid_history, mAP_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1:\n",
      "----------\n",
      "Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3356 [00:00<?, ?it/s]/home/avramidi/miniconda3/lib/python3.9/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      " 52%|█████▏    | 1760/3356 [08:16<07:29,  3.55it/s] \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 188.00 MiB (GPU 1; 10.76 GiB total capacity; 3.24 GiB already allocated; 171.44 MiB free; 3.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_55574/1280845313.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model, train_history, valid_history, mAP_history = train_model(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_map_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m )\n",
      "\u001b[0;32m/tmp/ipykernel_55574/732721424.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, best_model, train_loader, valid_loader, optim, epochs, path, evaluate_map_every)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mobjectness_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mrpn_box_reg_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         ) = train_one_epoch(model, train_loader, optim)\n\u001b[0m\u001b[1;32m     22\u001b[0m         train_history.update(\n\u001b[1;32m     23\u001b[0m             \u001b[0mtotal_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox_reg_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobjectness_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrpn_box_reg_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_55574/799346491.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, train_loader, optim)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mloss_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloss_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 188.00 MiB (GPU 1; 10.76 GiB total capacity; 3.24 GiB already allocated; 171.44 MiB free; 3.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "model, train_history, valid_history, mAP_history = train_model(\n",
    "    model, best_model, train_loader, valid_loader, optim, 1, path=path, evaluate_map_every=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3ab323e660f3a9bb361148c6889bf4609343562ee4930adaf3e4d1e531ca133b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
