{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "676fba20-0b0b-4296-ade8-3ef05fcd7e96",
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import os\n",
                "import matplotlib.pyplot as plt\n",
                "import pandas as pd\n",
                "import random\n",
                "import torch, torch.nn as nn\n",
                "from PIL import Image\n",
                "from tqdm import tqdm\n",
                "from torch.optim import AdamW\n",
                "from torch.utils import data\n",
                "from torchvision import transforms, models\n",
                "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
                "import torchvision\n",
                "from torchvision.utils import draw_bounding_boxes\n",
                "import torchvision.transforms.functional as F\n",
                "from torchmetrics.detection import MeanAveragePrecision\n",
                "import pickle"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "ac3c9353-e70d-470e-bce9-6587030cbab9",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "<torch._C.Generator at 0x7f6ce1bb4a30>"
                        ]
                    },
                    "execution_count": 2,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "DISABLE_TQDM = True\n",
                "torch.manual_seed(42)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "0041c2ee-51e4-4cbe-8a8b-386af5ac1d00",
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_test_split(path):\n",
                "    train_path = path + \"train_images/\"\n",
                "    train_list = [os.path.join(train_path, img) for img in os.listdir(train_path)]\n",
                "    random.shuffle(train_list)\n",
                "    threshold = int(0.8 * len(train_list))\n",
                "    valid_list = train_list[threshold:]\n",
                "    train_list = train_list[:threshold]\n",
                "    test_path = path + \"test_images/\"\n",
                "    test_list = [os.path.join(test_path, img) for img in os.listdir(test_path)]\n",
                "    return train_list, valid_list, test_list"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "id": "d6fad62e-90a2-4986-94c2-c526526255cb",
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_annotation_map(annotations):\n",
                "    annotation_map = {}\n",
                "    for _, row in annotations.iterrows():\n",
                "        boxes = annotation_map.get(row['image_id'], [])\n",
                "        boxes.append((row['lesion_type'], [row['xmin'] if row['xmin']>=0 else -1,\n",
                "                                           row['ymin'] if row['ymin']>=0 else -1,\\\n",
                "                                           row['xmax'] if row['xmax']>=0 else -1,\\\n",
                "                                           row['ymax'] if row['ymax']>=0 else -1\\\n",
                "                                           ]))\n",
                "        annotation_map[row['image_id']] = boxes\n",
                "    return annotation_map"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "186c90b8-3e12-4ca3-806c-e013b4f45089",
            "metadata": {},
            "outputs": [],
            "source": [
                "def split_annotation_map(annotation_map, train, valid):\n",
                "    \n",
                "    train_ids = [image.split('/')[-1].split('.')[0] for image in train]\n",
                "    valid_ids = [image.split('/')[-1].split('.')[0] for image in valid]\n",
                "\n",
                "    train_dict = {\n",
                "        image: annotation_map[image] for image in train_ids\n",
                "    }\n",
                "    valid_dict = {\n",
                "        image: annotation_map[image] for image in valid_ids\n",
                "    }\n",
                "    return train_dict, valid_dict"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "10f4934a-429c-4a6d-94a8-33f19205bb33",
            "metadata": {},
            "outputs": [],
            "source": [
                "class SpineObjectDetection(data.Dataset):\n",
                "    \n",
                "    def __init__(self, root_dir, annotation_map, anomaly_map, image_id_map, transform):\n",
                "        self.img_paths = root_dir\n",
                "        self.img_paths.sort()\n",
                "        self.image_ids = [image.split('/')[-1].split('.')[0] for image in self.img_paths]\n",
                "        \n",
                "        self.annotation_map = annotation_map\n",
                "        self.anomaly_map = anomaly_map\n",
                "        self.image_id_map = image_id_map\n",
                "        self.transform = transform       \n",
                "    \n",
                "    def __len__(self):\n",
                "        return len(self.img_paths)\n",
                "    \n",
                "    def __getitem__(self, idx):\n",
                "        image = Image.open(self.img_paths[idx])\n",
                "        image_id = self.image_ids[idx]\n",
                "        \n",
                "        image_width = image.size[0]\n",
                "        image_height = image.size[1]\n",
                "        \n",
                "        image = transforms.Resize(np.random.randint(640,800))(image)\n",
                "        transformed_w = image.size[0]\n",
                "        transformed_h = image.size[1]\n",
                "\n",
                "        labels, boxes, area = [], [], []\n",
                "        for label, box in self.annotation_map[image_id]:\n",
                "            labels.append(self.anomaly_map[label])\n",
                "            \n",
                "            if box[0] == -1 and box[1] == -1:\n",
                "                boxes.append([0,0,transformed_w,transformed_h])\n",
                "            else:\n",
                "                boxes.append([\n",
                "                    (box[0]/image_width)  * transformed_w,\n",
                "                    (box[1]/image_height) * transformed_h,\n",
                "                    (box[2]/image_width)  * transformed_w,\n",
                "                    (box[3]/image_height) * transformed_h\n",
                "                ])\n",
                "\n",
                "            area.append((boxes[-1][2] - boxes[-1][0]) * (boxes[-1][3] - boxes[-1][1]))\n",
                "\n",
                "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
                "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
                "        area = torch.as_tensor(area, dtype=torch.float32)       \n",
                "        \n",
                "        image, boxes = self.transform(image, boxes)\n",
                "        target = {\n",
                "            \"boxes\": boxes,\n",
                "            \"labels\": labels,\n",
                "            \"area\": area,\n",
                "            \"image_id\": torch.tensor(self.image_id_map[image_id])\n",
                "        }\n",
                "        \n",
                "        return image, target"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "0f860319-4f60-4067-81b8-8e34b3d176c8",
            "metadata": {},
            "outputs": [],
            "source": [
                "class Compose:\n",
                "    def __init__(self, transforms):\n",
                "        self.transforms = transforms\n",
                "\n",
                "    def __call__(self, img, boxes):\n",
                "        for t in self.transforms:\n",
                "            img, boxes = t(img,boxes)\n",
                "        return img, boxes\n",
                "\n",
                "    def __repr__(self) -> str:\n",
                "        format_string = self.__class__.__name__ + \"(\"\n",
                "        for t in self.transforms:\n",
                "            format_string += \"\\n\"\n",
                "            format_string += f\"    {t}\"\n",
                "        format_string += \"\\n)\"\n",
                "        return format_string\n",
                "\n",
                "class RandomHorizontalFlip():\n",
                "    \n",
                "    def __init__(self, p=0.5):\n",
                "        super().__init__()\n",
                "        self.p = p\n",
                "\n",
                "    def __call__(self, img, boxes):\n",
                "        if torch.rand(1) < self.p:\n",
                "            \n",
                "            img_width = img.size[0]\n",
                "            \n",
                "            boxes[:,0] = img_width - boxes[:,0]\n",
                "            boxes[:,2] = img_width - boxes[:,2]\n",
                "            boxes_w = torch.abs(boxes[:,0] - boxes[:,2])\n",
                "             \n",
                "            boxes[:,0] -= boxes_w\n",
                "            boxes[:,2] += boxes_w\n",
                "\n",
                "            return F.hflip(img),boxes\n",
                "        return img,boxes\n",
                "\n",
                "    def __repr__(self) -> str:\n",
                "        return f\"{self.__class__.__name__}(p={self.p})\"\n",
                "    \n",
                "class ToTensor:\n",
                "    def __call__(self, img, boxes):\n",
                "        return F.to_tensor(img), boxes\n",
                "\n",
                "    def __repr__(self) -> str:\n",
                "        return f\"{self.__class__.__name__}()\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "04d14dd0-ab9d-40b9-9662-342bb56ceb3b",
            "metadata": {},
            "outputs": [],
            "source": [
                "def collate_fn(batch):\n",
                "    return tuple(zip(*batch))\n",
                "\n",
                "def create_faster_rcnn_model(num_classes, trainable_backbone_layers=3):\n",
                "    model = models.detection.fasterrcnn_resnet50_fpn(\n",
                "        pretrained=True,\n",
                "        trainable_backbone_layers=trainable_backbone_layers,\n",
                "        min_size=640,\n",
                "        max_size=2699\n",
                "    )\n",
                "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
                "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes) \n",
                "    return model\n",
                "\n",
                "def show(imgs):\n",
                "    if not isinstance(imgs, list):\n",
                "        imgs = [imgs]\n",
                "    _, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
                "    for i, img in enumerate(imgs):\n",
                "        img = img.detach()\n",
                "        img = F.to_pil_image(img)\n",
                "        axs[0, i].imshow(np.asarray(img))\n",
                "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
                "        \n",
                "def show_tranformed_image(loader, number):\n",
                "    images, targets = next(iter(loader))\n",
                "    #images = [image for image in images]\n",
                "    targets = [{k: v for k, v in t.items()} for t in targets]\n",
                "    for image, target in list(zip(images,targets))[:number]:\n",
                "        result = draw_bounding_boxes(\n",
                "            (image*255).byte(),\n",
                "            target['boxes'],\n",
                "            width=5,\n",
                "            colors=['white']*len(target['boxes'])\n",
                "        )\n",
                "        show(result)\n",
                "\n",
                "def save_object(path,file_name,obj):\n",
                "    with open(os.path.join(path,file_name + \".pickle\"), 'wb') as file:\n",
                "        pickle.dump(obj, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
                "\n",
                "def load_object(path,file_name):\n",
                "    with open(os.path.join(path,file_name + \".pickle\"), 'rb') as file:\n",
                "        return pickle.load(file)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "e63e1e40-a5d9-4ef2-93b2-bc32518335cf",
            "metadata": {},
            "outputs": [],
            "source": [
                "class SaveBestModel:\n",
                "    def __init__(self, model_name, path=\"/\"):\n",
                "        self.best_valid_loss = float('inf')\n",
                "        self.path = path\n",
                "        self.model_name = model_name\n",
                "        \n",
                "    def update(self, model, current_valid_loss):\n",
                "        if current_valid_loss < self.best_valid_loss:\n",
                "            self.best_valid_loss = current_valid_loss\n",
                "            torch.save(model, os.path.join(self.path,self.model_name))\n",
                "            print(f\"Saved Model. Best validation loss: {self.best_valid_loss}\")\n",
                "            \n",
                "    def fetch(self):\n",
                "        return torch.load(\n",
                "            os.path.join(self.path, self.model_name)\n",
                "        )\n",
                "    \n",
                "class LossHistory:\n",
                "    def __init__(self,path,file_name):\n",
                "        self.loss = {\n",
                "            'total_loss': [],\n",
                "            'classifier_loss' : [],\n",
                "            'box_reg_loss':[],\n",
                "            'objectness_loss': [],\n",
                "            'rpn_box_reg_loss':[]\n",
                "        }\n",
                "        self.file_name = file_name\n",
                "        self.path = path\n",
                "    \n",
                "    def update(self,total_loss,classifier_loss,box_reg_loss,objectness_loss,rpn_box_reg_loss):\n",
                "        self.loss['total_loss'].append(total_loss)\n",
                "        self.loss['classifier_loss'].append(classifier_loss)\n",
                "        self.loss['box_reg_loss'].append(box_reg_loss)\n",
                "        self.loss['objectness_loss'].append(objectness_loss)\n",
                "        self.loss['rpn_box_reg_loss'].append(rpn_box_reg_loss)\n",
                "        \n",
                "    def save(self):\n",
                "        save_object(self.path, self.file_name, self.loss)\n",
                "    \n",
                "    def load(self):\n",
                "        self.loss = load_object(self.path, self.file_name)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "id": "4d635da1-9a08-4a25-962d-50b435118c7a",
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_one_epoch(model, train_loader, optim):\n",
                "\n",
                "    running_loss = {\n",
                "        \"running_total_loss\": 0,\n",
                "        \"running_loss_classifier\": 0,\n",
                "        \"running_loss_box_reg\": 0,\n",
                "        \"running_loss_objectness\": 0,\n",
                "        \"running_loss_rpn_box_reg\": 0\n",
                "    }\n",
                "\n",
                "    for images, targets in tqdm(train_loader, disable=DISABLE_TQDM):\n",
                "\n",
                "        images_device = list(image.to(device) for image in images)\n",
                "        targets_device = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
                "\n",
                "        optim.zero_grad()\n",
                "        with torch.set_grad_enabled(True):\n",
                "            loss_dict = model(images_device, targets_device)\n",
                "            loss = sum(loss for loss in loss_dict.values())\n",
                "            loss.backward()\n",
                "            optim.step()\n",
                "\n",
                "        running_loss[\"running_total_loss\"] += loss.item() / len(train_loader)\n",
                "        running_loss[\"running_loss_classifier\"] += loss_dict['loss_classifier'].item() / len(train_loader)\n",
                "        running_loss[\"running_loss_box_reg\"] += loss_dict['loss_box_reg'].item() / len(train_loader)\n",
                "        running_loss[\"running_loss_objectness\"] += loss_dict['loss_objectness'].item() / len(train_loader)\n",
                "        running_loss[\"running_loss_rpn_box_reg\"] += loss_dict['loss_rpn_box_reg'].item() / len(train_loader)\n",
                "\n",
                "    #epoch_total_loss = running_loss[\"running_total_loss\"] / len(train_loader)\n",
                "    #epoch_loss_classifier = running_loss[\"running_loss_classifier\"] / len(train_loader)\n",
                "    #epoch_loss_box_reg = running_loss[\"running_loss_box_reg\"] / len(train_loader)\n",
                "    #epoch_loss_objectness = running_loss[\"running_loss_objectness\"] / len(train_loader)\n",
                "    #epoch_loss_rpn_box_reg = running_loss[\"running_loss_rpn_box_reg\"] / len(train_loader)\n",
                "\n",
                "    print(running_loss)\n",
                "    return running_loss\n",
                "\n",
                "def evaluate_loss(model, loader):\n",
                "\n",
                "    running_loss = {\n",
                "        \"running_total_loss\": 0,\n",
                "        \"running_loss_classifier\": 0,\n",
                "        \"running_loss_box_reg\": 0,\n",
                "        \"running_loss_objectness\": 0,\n",
                "        \"running_loss_rpn_box_reg\": 0\n",
                "    }\n",
                "\n",
                "    for images, targets in tqdm(loader, disable=DISABLE_TQDM):\n",
                "        images_device = list(image.to(device) for image in images)\n",
                "        targets_device = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
                "        model.train()\n",
                "        with torch.set_grad_enabled(False):\n",
                "            loss_dict = model.forward(images_device,targets_device)\n",
                "            loss = sum(loss for loss in loss_dict.values())\n",
                "\n",
                "        running_loss[\"running_total_loss\"] += loss.item() / len(train_loader)\n",
                "        running_loss[\"running_loss_classifier\"] += loss_dict['loss_classifier'].item() / len(train_loader)\n",
                "        running_loss[\"running_loss_box_reg\"] += loss_dict['loss_box_reg'].item() / len(train_loader)\n",
                "        running_loss[\"running_loss_objectness\"] += loss_dict['loss_objectness'].item() / len(train_loader)\n",
                "        running_loss[\"running_loss_rpn_box_reg\"] += loss_dict['loss_rpn_box_reg'].item() / len(train_loader)\n",
                "\n",
                "    print(running_loss)\n",
                "    return running_loss\n",
                "\n",
                "def evaluate_average_precision(model,loader):\n",
                "    metric = MeanAveragePrecision(class_metrics=True)\n",
                "    for images, targets in tqdm(loader, disable=DISABLE_TQDM):\n",
                "        images_device = list(image.to(device) for image in images)\n",
                "        targets_device = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
                "        model.eval()\n",
                "        with torch.set_grad_enabled(False):\n",
                "            preds = model.forward(images_device)\n",
                "            metric.update(preds, targets_device)\n",
                "    mAP =  metric.compute()\n",
                "    print(mAP)\n",
                "    return mAP"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "id": "75aabfcd-3716-4271-925b-1cc4e025a01d",
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_model(model, best_model, train_loader,valid_loader, optim, path=\".\",evaluate_map_every=5):\n",
                "\n",
                "    model.train()\n",
                "\n",
                "    train_history = LossHistory(path, \"train_history\")\n",
                "    valid_history = LossHistory(path, \"valid_history\")\n",
                "    mAP_history = []\n",
                "    \n",
                "    for i in range(1, n_epochs+1):\n",
                "        print(f\"\\nEpoch {i}:\")\n",
                "        print(\"-\"*100)\n",
                "\n",
                "        ## Training\n",
                "        print(\"Train\")\n",
                "        total_loss,classifier_loss,box_reg_loss,objectness_loss, rpn_box_reg_loss = train_one_epoch(model,train_loader,optim)\n",
                "        train_history.update(total_loss,classifier_loss,box_reg_loss,objectness_loss, rpn_box_reg_loss)\n",
                "        \n",
                "        ## Validation\n",
                "        print(\"\\nValidation\")\n",
                "        total_loss,classifier_loss,box_reg_loss,objectness_loss, rpn_box_reg_loss = evaluate_loss(model,valid_loader)\n",
                "        valid_history.update(total_loss,classifier_loss,box_reg_loss,objectness_loss, rpn_box_reg_loss)\n",
                "        best_model.update(model,total_loss)\n",
                "        \n",
                "        ## Validation Eval mAP\n",
                "        print(\"\\nmAP Validation\")\n",
                "        if i%evaluate_map_every ==0:\n",
                "            mAP = evaluate_average_precision(model,valid_loader)\n",
                "            mAP_history.append(mAP)\n",
                "            \n",
                "        train_history.save(),\n",
                "        valid_history.save()\n",
                "        save_object(path,\"map_history\", mAP_history)\n",
                "        \n",
                "    return best_model.fetch(),train_history,valid_history,mAP_history"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "id": "6fe6a278-6558-4ace-8a09-c906ee8fe2c5",
            "metadata": {},
            "outputs": [],
            "source": [
                "path = '/data/avramidi/tiny_vindr/'\n",
                "train_path = path + \"train_images/\"\n",
                "test_path  = path + \"test_images/\"\n",
                "annot_path = path + \"annotations/\"\n",
                "\n",
                "anomaly_map = {\n",
                "    'No finding': 0,\n",
                "    'Disc space narrowing': 1,\n",
                "    'Foraminal stenosis': 2,\n",
                "    'Osteophytes': 3,\n",
                "    'Spondylolysthesis': 4,\n",
                "    'Surgical implant': 5,\n",
                "    'Vertebral collapse': 6,   \n",
                "    'Other lesions': 7,\n",
                " }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "id": "296e1606-889a-408d-b3d6-313d07205902",
            "metadata": {},
            "outputs": [],
            "source": [
                "train, valid, test = train_test_split(path)\n",
                "\n",
                "train_annotations = pd.read_csv(annot_path + \"train.csv\")\n",
                "test_annotations = pd.read_csv(annot_path + \"test.csv\")\n",
                "\n",
                "train_annotation_map = compute_annotation_map(train_annotations)\n",
                "train_annotation_map, valid_annotation_map = split_annotation_map(train_annotation_map, train, valid)\n",
                "\n",
                "test_annotation_map = compute_annotation_map(test_annotations)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "id": "926ebe3d-ea47-40e2-889e-a9e213441ac4",
            "metadata": {},
            "outputs": [],
            "source": [
                "all_image_ids = [image.split('/')[-1].split('.')[0] for image in train] + \\\n",
                "                [image.split('/')[-1].split('.')[0] for image in valid] + \\\n",
                "                [image.split('/')[-1].split('.')[0] for image in test]\n",
                "\n",
                "image_id_map = {\n",
                "    img_id: i+1 for i, img_id in enumerate(all_image_ids)\n",
                "}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "id": "cd0b3d5e-1f7a-4702-95ba-2e43fb77dae6",
            "metadata": {},
            "outputs": [],
            "source": [
                "train_transform = Compose([\n",
                "    RandomHorizontalFlip(p=0.5),\n",
                "    ToTensor()\n",
                "])\n",
                "test_transform = Compose([\n",
                "    ToTensor()\n",
                "])\n",
                "\n",
                "train_dataset = SpineObjectDetection(train, train_annotation_map, anomaly_map, image_id_map, train_transform)\n",
                "valid_dataset = SpineObjectDetection(valid, valid_annotation_map, anomaly_map, image_id_map, train_transform)\n",
                "test_dataset  = SpineObjectDetection(test , test_annotation_map , anomaly_map, image_id_map, test_transform)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "id": "2a3fe20c-3312-4d83-a617-2b2bd5860080",
            "metadata": {},
            "outputs": [],
            "source": [
                "batch_size = 16\n",
                "\n",
                "train_loader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, num_workers=8)\n",
                "valid_loader = data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, num_workers=8)\n",
                "test_loader  = data.DataLoader(test_dataset , batch_size=batch_size, shuffle=True, collate_fn=collate_fn, num_workers=8)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "id": "35ee4052-faef-42ee-a9b7-df729a61afb0",
            "metadata": {},
            "outputs": [],
            "source": [
                "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
                "model = create_faster_rcnn_model(num_classes=8, trainable_backbone_layers=3).to(device)\n",
                "optim = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)\n",
                "n_epochs = 1\n",
                "best_model = SaveBestModel(\"model_object.pt\",path)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "id": "6c4480e3-3841-4d3b-b333-f1a1a8656cbf",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Epoch 1:\n",
                        "----------------------------------------------------------------------------------------------------\n",
                        "Train\n"
                    ]
                },
                {
                    "ename": "RuntimeError",
                    "evalue": "CUDA out of memory. Tried to allocate 364.00 MiB (GPU 1; 10.76 GiB total capacity; 9.01 GiB already allocated; 31.44 MiB free; 9.57 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
                        "\u001b[0;32m/tmp/ipykernel_1197/1918671913.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model, train_history, valid_history, mAP_history = train_model(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_map_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m )\n",
                        "\u001b[0;32m/tmp/ipykernel_1197/1268905923.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, best_model, train_loader, valid_loader, optim, path, evaluate_map_every)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m## Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclassifier_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbox_reg_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mobjectness_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrpn_box_reg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mtrain_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclassifier_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbox_reg_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mobjectness_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrpn_box_reg_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/tmp/ipykernel_1197/2146016755.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, train_loader, optim)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mloss_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloss_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torchvision/models/detection/generalized_rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     91\u001b[0m                                      .format(degen_bb, target_idx))\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torchvision/models/detection/backbone_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfpn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torchvision/models/_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mout_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownsample\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torchvision/ops/misc.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mscale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrv\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrsqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mrm\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mscale\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 364.00 MiB (GPU 1; 10.76 GiB total capacity; 9.01 GiB already allocated; 31.44 MiB free; 9.57 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
                    ]
                }
            ],
            "source": [
                "model, train_history, valid_history, mAP_history = train_model(\n",
                "    model, best_model, train_loader, valid_loader, optim, path=path, evaluate_map_every=1\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 37,
            "id": "cf7b8283-98fc-47cc-b96f-11426b1b0918",
            "metadata": {},
            "outputs": [],
            "source": [
                "model = torch.load(path + \"model_object.pt\")\n",
                "results = evaluate_average_precision(model,test_loader)\n",
                "results"
            ]
        }
    ],
    "metadata": {
        "interpreter": {
            "hash": "3ab323e660f3a9bb361148c6889bf4609343562ee4930adaf3e4d1e531ca133b"
        },
        "kernelspec": {
            "display_name": "Python 3.7.6",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}